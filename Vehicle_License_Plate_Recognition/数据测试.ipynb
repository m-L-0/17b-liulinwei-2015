{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0  TEST ACCURACY: 0.200 \n",
      "step: 100  TEST ACCURACY: 0.600 \n",
      "step: 200  TEST ACCURACY: 0.850 \n",
      "step: 300  TEST ACCURACY: 1.000 \n",
      "step: 400  TEST ACCURACY: 0.950 \n",
      "step: 500  TEST ACCURACY: 1.000 \n",
      "step: 600  TEST ACCURACY: 1.000 \n",
      "step: 700  TEST ACCURACY: 1.000 \n",
      "step: 800  TEST ACCURACY: 1.000 \n",
      "step: 900  TEST ACCURACY: 0.950 \n",
      "step: 1000  TEST ACCURACY: 1.000 \n",
      "step: 1100  TEST ACCURACY: 0.950 \n",
      "step: 1200  TEST ACCURACY: 1.000 \n",
      "step: 1300  TEST ACCURACY: 1.000 \n",
      "step: 1400  TEST ACCURACY: 1.000 \n",
      "step: 1500  TEST ACCURACY: 1.000 \n",
      "step: 1600  TEST ACCURACY: 1.000 \n",
      "step: 1700  TEST ACCURACY: 1.000 \n",
      "step: 1800  TEST ACCURACY: 1.000 \n",
      "step: 1900  TEST ACCURACY: 1.000 \n",
      "TEST ACCURACY: 0.925\n",
      "标签5的召回率是：0.952\n",
      "标签7的召回率是：0.941\n",
      "标签1的召回率是：0.950\n",
      "标签12的召回率是：0.909\n",
      "标签4的召回率是：0.934\n",
      "标签9的召回率是：0.948\n",
      "标签3的召回率是：0.953\n",
      "标签11的召回率是：0.867\n",
      "标签8的召回率是：0.927\n",
      "标签2的召回率是：0.958\n",
      "标签10的召回率是：0.975\n",
      "标签0的召回率是：0.919\n",
      "标签13的召回率是：0.797\n",
      "标签6的召回率是：0.954\n",
      "标签14的召回率是：0.977\n",
      "标签15的召回率是：0.838\n",
      "标签17的召回率是：0.949\n",
      "标签16的召回率是：0.863\n",
      "标签18的召回率是：0.930\n",
      "标签19的召回率是：0.911\n",
      "标签20的召回率是：0.942\n",
      "标签21的召回率是：0.961\n",
      "标签22的召回率是：0.907\n",
      "标签23的召回率是：0.932\n",
      "标签24的召回率是：0.958\n",
      "标签25的召回率是：0.917\n",
      "标签26的召回率是：0.931\n",
      "标签27的召回率是：0.937\n",
      "标签28的召回率是：0.889\n",
      "标签30的召回率是：0.942\n",
      "标签29的召回率是：0.930\n",
      "标签31的召回率是：0.993\n",
      "标签32的召回率是：0.978\n",
      "标签33的召回率是：0.976\n"
     ]
    }
   ],
   "source": [
    "# 卷积神经网络\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def read_and_decode(filename):\n",
    "    #根据文件名生成一个队列\n",
    "    filename_queue = tf.train.string_input_producer([filename]) #选择要读取的文件的名字，用 tf.train.string_input_producer 函数来生成文件名队列，这个函数可以设置shuffle = Ture，来打乱队列，可以设置epoch = x，过x遍训练数据。\n",
    "    reader = tf.TFRecordReader() #文件读取器\n",
    "    _, serialized_example = reader.read(filename_queue)  #返回文件名和文件\n",
    "    features = tf.parse_single_example(serialized_example, #通过解析器tf.parse_single_example解析\n",
    "                                       features={\n",
    "                                       'label': tf.FixedLenFeature([], tf.int64),\n",
    "                                       'img_raw' : tf.FixedLenFeature([], tf.string),\n",
    "                                       })\n",
    "    image = tf.decode_raw(features['img_raw'], tf.uint8) #用解码器 tf.decode_raw 解码。\n",
    "    image = tf.cast(image, dtype='float32')*(1/255)  # 归一化处理\n",
    "    image = tf.reshape(image, [48, 24, 3]) # 恢复数据形状\n",
    "    image = tf.split(image, 3, 2)[0]\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    return image, label\n",
    "\n",
    "# 定义好初始化函数以便重复使用。给权重制造一些随机噪声来打破完全对称，使用截断的正态分布，标准差设为0.1，\n",
    "# 同时因为使用relu，也给偏执增加一些小的正值(0.1)用来避免死亡节点(dead neurons)\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') # 参数分别指定了卷积核的尺寸、多少个channel、filter的个数即产生特征图的个数\n",
    "\n",
    "# 2x2最大池化，即将一个2x2的像素块降为1x1的像素。最大池化会保留原始像素块中灰度值最高的那一个像素，即保留最显著的特征。\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "n_input  = 1152 # 24*48的灰度图，像素个数1152\n",
    "# 在设计网络结构前，先定义输入的placeholder，x是特征，y是真实的label\n",
    "x = tf.placeholder(tf.float32, [None,n_input]) \n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "x_image = tf.reshape(x, [-1, 48, 24, 1]) # 对图像做预处理，将1D的输入向量转为2D的图片结构，即1*1152到24*48的结构,-1代表样本数量不固定，1代表颜色通道数量\n",
    "\n",
    "#------------------------------------------------\n",
    "#定义CNN结构\n",
    "#------------------------------------------------\n",
    "\n",
    "# 定义第一个卷积层，使用前面写好的函数进行参数初始化，包括weight和bias\n",
    "W_conv1 = weight_variable([3, 3, 1, 32])  # 前两个维度代表了过滤器的尺寸，第三个维度表示当前曾的深度，第四个维度表示过滤器的深度。\n",
    "b_conv1 = bias_variable([32])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "#-1*48*24*32\n",
    "#池化\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "#-1*24*12*32\n",
    "\n",
    "# 定义第二个卷积层\n",
    "W_conv2 = weight_variable([3, 3, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#-1*24*12*64\n",
    "#池化\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "#-1*12*6*96\n",
    "\n",
    "#定义第三个卷积层\n",
    "W_conv3 = weight_variable([3, 3, 64, 96])\n",
    "b_conv3 = bias_variable([96])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "#-1*12*6*96\n",
    "#池化\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "#-1*6*3*96\n",
    "\n",
    "# 全连接层\n",
    "W_fc1 = weight_variable([6*3*96, 512])  ###++仔细算下卷积-池化-卷积-池化后得到的shape\n",
    "b_fc1 = bias_variable([512])\n",
    "h_pool2_flat = tf.reshape(h_pool3, [-1, 6*3*96])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "###--正则化--\n",
    "# 为了减轻过拟合，使用Dropout层\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# Dropout层输出连接一个Softmax层,得到最后的概率输出\n",
    "W_fc2 = weight_variable([512, 34])\n",
    "b_fc2 = bias_variable([34])\n",
    "\n",
    "pred_not_softmax=tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred_not_softmax,labels=y)\n",
    "cost=tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#输出层和交叉熵的定义\n",
    "out_op=tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "#预测正确率\n",
    "d = tf.argmax(pred_not_softmax, 1)\n",
    "corr = tf.equal(tf.argmax(pred_not_softmax, 1), y) # 对比预测值的索引和真实label的索引是否一样，一样返回True，不一样返回False\n",
    "accuracy = tf.reduce_mean(tf.cast(corr, tf.float32))\n",
    "\n",
    "\n",
    "# #读取训练集数据\n",
    "# image, label = read_and_decode(\"train.tfrecords\")\n",
    "# batch_size=20\n",
    "# image_batch, label_batch = tf.train.shuffle_batch([image,label], batch_size=20, capacity=2000, min_after_dequeue=1900, num_threads=2)   #每进行一次迭代选择20个样本\n",
    "\n",
    "# #读取验证集数据\n",
    "# vimage, vlabel = read_and_decode(\"validation.tfrecords\")\n",
    "# vimage_batch, vlabel_batch = tf.train.shuffle_batch([vimage,vlabel], batch_size=20, capacity=2000, min_after_dequeue=1900, num_threads=2)   #每进行一次迭代选择100个样本\n",
    "\n",
    "\n",
    "#读取测试数据\n",
    "Timage, Tlabel = read_and_decode(\"test.tfrecords\")\n",
    "batch_size=20\n",
    "Timage_batch, Tlabel_batch = tf.train.shuffle_batch([Timage,Tlabel], batch_size=20, capacity=2000, min_after_dequeue=1900, num_threads=2)   #每进行一次迭代选择20个样本\n",
    "\n",
    "# 初始化所有参数\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "a = 0\n",
    "a1 = 0\n",
    "a2 = 0\n",
    "b = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator() #创建一个协调器，管理线程\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)   #启动QueueRunner, 此时文件名队列已经进队\n",
    "    new_dict = {}  # 定义一个字典\n",
    "    for i in range(2000):  # 规定出队数量        \n",
    "        Timg, Tlabel = sess.run([Timage_batch, Tlabel_batch])\n",
    "        Timg = np.reshape(Timg, [-1,1152])  #对img的reshape做了修改\n",
    "        Tlabel = np.reshape(Tlabel,[-1])    #对label的reshape做了修改\n",
    "        out_op.run(feed_dict={x:Timg, y:Tlabel, keep_prob:0.7})\n",
    "        # 召回率\n",
    "        c, lab = sess.run([corr, y], feed_dict={x:Timg, y:Tlabel, keep_prob:1.0})\n",
    "        for j in range(len(c)):\n",
    "            if lab[j] in new_dict:\n",
    "                if c[j] == True:\n",
    "                    new_dict[lab[j]][0] += 1\n",
    "                    new_dict[lab[j]][1] += 1\n",
    "                else:\n",
    "                    new_dict[lab[j]][1] += 1\n",
    "            else:\n",
    "                if c[j] == True:\n",
    "                    new_dict[lab[j]] = [1, 1]\n",
    "                else:\n",
    "                    new_dict[lab[j]] = [0, 1]   \n",
    "        if i % 100 == 0: # 每100次训练，对准确率进行一次测试\n",
    "            \n",
    "            test_accuracy = accuracy.eval(feed_dict = {x:Timg, y:Tlabel, keep_prob:1.0}) #测试集精准度\n",
    "\n",
    "            print(\"step: %d  TEST ACCURACY: %.3f \" %(i, test_accuracy)) \n",
    "            a2 += test_accuracy\n",
    "            b += 1\n",
    "    print('TEST ACCURACY: %.3f' %(a2/b))\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    for k in new_dict:\n",
    "        print(\"标签%s的召回率是：%.3f\" %(k,  new_dict[k][0]/new_dict[k][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
